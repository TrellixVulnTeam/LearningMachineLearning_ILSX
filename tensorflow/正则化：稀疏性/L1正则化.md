# 稀疏性正则化 (Regularization for Sparsity)：L₁ 正则化



稀疏矢量通常包含许多维度。创建[特征组合](https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture)会导致包含更多维度。由于使用此类高维度特征矢量，因此模型可能会非常庞大，并且需要大量的 RAM。

在高维度稀疏矢量中，最好尽可能使权重正好降至 `0`。正好为 0 的权重基本上会使相应特征从模型中移除。 将特征设为 0 可节省 RAM 空间，且可以减少模型中的噪点。

以一个涵盖全球地区（不仅仅只是涵盖加利福尼亚州）的住房数据集为例。如果按分（每度为 60 分）对全球纬度进行分桶，则在一次稀疏编码过程中会产生大约 1 万个维度；如果按分对全球经度进行分桶，则在一次稀疏编码过程中会产生大约 2 万个维度。这两种特征的特征组合会产生大约 2 亿个维度。这 2 亿个维度中的很多维度代表非常有限的居住区域（例如海洋里），很难使用这些数据进行有效泛化。 若为这些不需要的维度支付 RAM 存储费用就太不明智了。 因此，最好是使无意义维度的权重正好降至 0，这样我们就可以避免在推理时支付这些模型系数的存储费用。

我们或许可以添加适当选择的正则化项，将这种想法变成在训练期间解决的优化问题。

L2 正则化能完成此任务吗？遗憾的是，不能。 L2 正则化可以使权重变小，但是并不能使它们正好为 0.0。

另一种方法是尝试创建一个正则化项，减少模型中的非零系数值的计数。只有在模型能够与数据拟合时增加此计数才有意义。 遗憾的是，虽然这种基于计数的方法看起来很有吸引力，但它会将我们的凸优化问题变为非凸优化问题，即 [NP 困难](https://wikipedia.org/wiki/NP-hardness)。 （如果您仔细观察，便会发现它与背包问题关联。） 因此，L0 正则化这种想法在实践中并不是一种有效的方法。

不过，L1 正则化这种正则化项的作用类似 L0，但它具有凸优化的优势，可有效进行计算。因此，我们可以使用 L1 正则化使模型中很多信息缺乏的系数正好为 0，从而在推理时节省 RAM。

## L1 和 L2 正则化。

L2 和 L1 采用不同的方式降低权重：

- L2 会降低权重2。
- L1 会降低 |权重|。

因此，L2 和 L1 具有不同的导数：

- L2 的导数为 2 * 权重。
- L1 的导数为 k（一个常数，其值与权重无关）。

您可以将 L2 的导数的作用理解为每次移除权重的 x%。如 [Zeno](https://wikipedia.org/wiki/Zeno%27s_paradoxes#Dichotomy_paradox) 所知，对于任意数字，即使按每次减去 x% 的幅度执行数十亿次减法计算，最后得出的值也绝不会正好为 0。（Zeno 不太熟悉浮点精度限制，它可能会使结果正好为 0。）总而言之，L2 通常不会使权重变为 0。

您可以将 L1 的导数的作用理解为每次从权重中减去一个常数。不过，由于减去的是绝对值，L1 在 0 处具有不连续性，这会导致与 0 相交的减法结果变为 0。例如，如果减法使权重从 +0.1 变为 -0.2，L1 便会将权重设为 0。就这样，L1 使权重变为 0 了。

L1 正则化 - 减少所有权重的绝对值 - 证明对宽度模型非常有效。

请注意，该说明适用于一维模型。

点击下面的“播放”按钮 (play_arrow)，比较 L1 和 L2 正则化对权重网络的影响。



<iframe scrolling="no" class="inherit-locale" frameborder="0" style="height: 560px; width: 100%; padding: 30px 0;" src="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/reg-compare.jshtml?hl=zh-cn"></iframe>

