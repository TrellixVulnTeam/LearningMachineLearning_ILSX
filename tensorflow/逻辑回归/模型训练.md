# 逻辑回归 (Logistic Regression)：模型训练



## 逻辑回归的损失函数

线性回归的损失函数是平方损失。逻辑回归的损失函数是**对数损失函数**，定义如下：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>L</mi>
  <mi>o</mi>
  <mi>g</mi>
  <mi>L</mi>
  <mi>o</mi>
  <mi>s</mi>
  <mi>s</mi>
  <mo>=</mo>
  <munder>
​    <mo>&#x2211;<!-- ∑ --></mo>
​    <mrow class="MJX-TeXAtom-ORD">
​      <mo stretchy="false">(</mo>
​      <mi>x</mi>
​      <mo>,</mo>
​      <mi>y</mi>
​      <mo stretchy="false">)</mo>
​      <mo>&#x2208;<!-- ∈ --></mo>
​      <mi>D</mi>
​    </mrow>
  </munder>
  <mo>&#x2212;<!-- − --></mo>
  <mi>y</mi>
  <mi>l</mi>
  <mi>o</mi>
  <mi>g</mi>
  <mo stretchy="false">(</mo>
  <msup>
​    <mi>y</mi>
​    <mo>&#x2032;</mo>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>&#x2212;<!-- − --></mo>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;<!-- − --></mo>
  <mi>y</mi>
  <mo stretchy="false">)</mo>
  <mi>l</mi>
  <mi>o</mi>
  <mi>g</mi>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;<!-- − --></mo>
  <msup>
​    <mi>y</mi>
​    <mo>&#x2032;</mo>
  </msup>
  <mo stretchy="false">)</mo>
</math>



其中：

- (*x**y*)ϵ*D* 是包含很多有标签样本 (*x*,*y*) 的数据集。
- “y”是有标签样本中的标签。由于这是逻辑回归，因此“y”的每个值必须是 0 或 1。
- “y'”是对于特征集“x”的预测值（介于 0 和 1 之间）。

对数损失函数的方程式与 [Shannon 信息论中的熵测量](https://wikipedia.org/wiki/Entropy_(information_theory))密切相关。它也是[似然函数](https://wikipedia.org/wiki/Likelihood_function)的负对数（假设“y”属于[伯努利分布](https://wikipedia.org/wiki/Bernoulli_distribution)）。实际上，最大限度地降低损失函数的值会生成最大的似然估计值。



## 逻辑回归中的正则化

正则化在逻辑回归建模中极其重要。如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：

- L2 正则化。
- 早停法，即，限制训练步数或学习速率。

（我们会在之后的单元中讨论第三个策略，即 L1 正则化。）

假设您向每个样本分配一个唯一 ID，且将每个 ID 映射到其自己的特征。如果您未指定正则化函数，模型会变得完全过拟合。这是因为模型会尝试促使所有样本的损失达到 0 但始终达不到，从而使每个指示器特征的权重接近正无穷或负无穷。当有大量罕见的特征组合且每个样本中仅一个时，包含特征组合的高维度数据会出现这种情况。

幸运的是，使用 L2 或早停法可以防止出现此类问题。